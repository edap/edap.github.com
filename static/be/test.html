<!doctype html>
<html lang="en">

<head>
    <style>
        body{
            background-color: black;
            margin: 0px; padding: 0px; overflow: hidden;
        }
        #wait{
            color:white;
            font-family: system-ui;
            font-size: 42px;
            z-index: 3000;
            text-align: center;
            position: absolute;
            width: 100%;
            padding-top: 15%;
        }
        #video {
            position:absolute;
            top: -0;
            left: 0;
            right: 0;
            bottom: 0;
        }
    </style>
    <meta charset="utf-8">
    <title>Be the sun</title>
    <meta name="description"
        content="A once in a life-time occasion to be the sun, making sunsets and sunrises whenever you want">
    <meta name="author" content="Davide Prati">
    <!-- <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
	<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/facemesh"></script> -->
    <script src="./js/regl.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js" crossorigin="anonymous"></script>

    <!-- https://github.com/google/mediapipe/blob/master/docs/solutions/face_detection.md -->


<script type="module">
import * as THREE from 'https://threejsfundamentals.org/threejs/resources/threejs/r127/build/three.module.js';

const FACE_UP = 10
const FACE_DOWN = 152
const NOSE_TIP = 2
const VIDEO_WIDTH = 300
const VIDEO_HEIGHT = 225
// const VIDEO_WIDTH = 1400
// const VIDEO_HEIGHT = 1050
const SUN_SIZE_MAX = 1.0
const SUN_SIZE_MIN = 0.2

const videoElement = document.getElementById('video');
const canvasElement = document.getElementById('facecanvas');
const regl = true;

const faceMesh = new FaceMesh({
    locateFile: (file) => {
        return `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`;
    }
});

faceMesh.setOptions({
    maxNumFaces: 1,
    minDetectionConfidence: 0.5,
    minTrackingConfidence: 0.5
});


function initThreeApp(canvas, w, h) {
    const renderer = new THREE.WebGLRenderer({
        canvas,
        alpha: true,
        antialias: true,
    })

    const fov = 75
    const near = 0.01
    const far = 1000
    const camera = new THREE.PerspectiveCamera(fov, 1280/720, near, far)
    camera.position.z = 2;

    const scene = new THREE.Scene()

    function resize() {
        const width = w || window.innerWidth
        const height = h || window.innerHeight

        renderer.setSize(width, height)
        renderer.setPixelRatio(window.pixelRatio)

        if (camera.isPerspectiveCamera) {
            camera.aspect = width / height
        }
        camera.updateProjectionMatrix()
    }

    function render() {
        renderer.render(scene, camera)
    }

    // initial resize and render
    resize()
    render()


    // add a light
    const color = 0xFFFFFF
    const intensity = 1
    const light = new THREE.DirectionalLight(color, intensity)
    light.position.set(-1, 2, 4)
    scene.add(light)

    // add a box
    const boxWidth = 1
    const boxHeight = 1
    const boxDepth = 1
    const geometry = new THREE.BoxGeometry(boxWidth, boxHeight, boxDepth)
    const material = new THREE.MeshPhongMaterial({
        color: 0x44aa88,
        transparent: true,
        opacity: 0.8
    })
    const cube = new THREE.Mesh(geometry, material)
    scene.add(cube)

    return {
        renderer,
        camera,
        scene,
        resize,
        render,
        cube
    }
}


initVideo(videoElement, 320, 180)
const threeApp = initThreeApp(canvasElement,1280,720)

const onResults = function(res){
    console.log("calles")
  const landmarks = res.multiFaceLandmarks
  if(!landmarks) return;
  const {x, y, z} = landmarks[0][1];
  // landmarks[0][1] == nose position(face center point)
  // use landmarks xy value to calculate the screen xy
  let vec = new THREE.Vector3();
  let pos = new THREE.Vector3();
  vec.set(
    x * 2 - 1,
    -y * 2 + 1,
    0.5);
  vec.unproject(threeApp.camera);
  vec.sub(threeApp.camera.position).normalize();
  let distance = -threeApp.camera.position.z / vec.z;
  pos.copy(threeApp.camera.position).add(vec.multiplyScalar(distance));
  threeApp.cube.position.x = pos.x;
  threeApp.cube.position.y = pos.y;
  
  // todo
  // got the cube xy then how to get the z value?
  

}

faceMesh.onResults(onResults);

const run = async function(){
  threeApp.render()
  await faceMesh.send({image: videoElement})
  requestAnimationFrame(run)
}

function removeWaitingMessage(){
    let msg = document.getElementById('wait')
    msg.remove()
}

function initVideo(video, w, h){
  if ( navigator.mediaDevices && navigator.mediaDevices.getUserMedia ) {
    const constraints = { video: { width: w, height: h, facingMode: 'user' } };
    navigator.mediaDevices.getUserMedia( constraints ).then( function ( stream ) {
        // apply the stream to the video element used in the texture
        video.srcObject = stream;
        video.play();
        removeWaitingMessage()
        run();
    } ).catch( function ( error ) {
        console.error( 'Unable to access the camera/webcam.', error );
    } );
    } else {
        removeWaitingMessage()
        console.error( 'MediaDevices interface not available.' );
    }
}


</script>


</head>

<body>
    <div id="wait">Wait for camera to be detected, otherwise use the mouse as input</div>
    <!-- <video autoplay muted 
        playsinline hidden id="video"
        style=" top:0px; left: 500px"
        width="300" height="225"
        >
	</video> -->

    <video id="video"></video>
    <canvas id="facecanvas"></canvas>
</body>
</html>