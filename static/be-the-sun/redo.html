<!doctype html>
<html lang="en">

<head>
    <style>
        body{
            background-color: black;
            margin: 0px; padding: 0px; overflow: hidden;
        }
        #wait{
            color:white;
            font-family: system-ui;
            font-size: 42px;
            z-index: 3000;
            text-align: center;
            position: absolute;
            width: 100%;
            padding-top: 15%;
        }
        #video {
            position:absolute;
            top: -0;
            left: 0;
            right: 0;
            bottom: 0;
        }
    </style>
    <meta charset="utf-8">
    <title>Be the sun</title>
    <meta name="description" content="Be The Sun">
    <meta name="author" content="Davide Prati">
    <script src="./js/regl.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js" crossorigin="anonymous"></script>

    <script type="module">
    // 2017 first shadertoy version https://www.shadertoy.com/view/4lsyzn
    // 2017 webcam version online at edapx.neocities.org. defunct
    // 2018 online at davideprati.com, using headtracker.js
    // 2022 update. Use clmtracker.js
    // 2024 update. Use google mediapipe
    import vision from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3";
    const { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;

    const FACE_UP = 10
    const FACE_DOWN = 152
    const NOSE_TIP = 1
    const VIDEO_WIDTH = 300
    const VIDEO_HEIGHT = 225
    const SUN_SIZE_MAX = 1.0
    const SUN_SIZE_MIN = 0.2
    const DEBUG = true;

    let faceLandmarker;
    let runningMode = "VIDEO";
    let headPosition = [200.0, 200.0]
    let sunSize = 0.6
    const video = document.getElementById('video')
    const canvasElement = document.getElementById('facecanvas')
    canvasElement.width = window.innerWidth
    canvasElement.height = window.innerHeight

    if (!DEBUG) {
        video.setAttribute("hidden", "hidden")
    }


    initDemo(video, VIDEO_WIDTH, VIDEO_HEIGHT)

    // Before we can use FaceLandmarker class we must wait for it to finish
    // loading. Machine Learning models can be large and take a moment to
    // get everything needed to run.
    async function createFaceLandmarker() {
        const filesetResolver = await FilesetResolver.forVisionTasks(
            "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3/wasm"
        );
        faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {
            baseOptions: {
            modelAssetPath: `https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task`,
            delegate: "GPU"
            },
            outputFaceBlendshapes: true,
            runningMode,
            numFaces: 1
        });
    };



    function map_range(value, low1, high1, low2, high2) {
        let mapped = low2 + (high2 - low2) * (value - low1) / (high1 - low1)
        return Math.min(Math.max(mapped, low2), high2)
    }

    const onResults = function(res){
        const landmarks = res.multiFaceLandmarks
        if(!landmarks || !landmarks[0]) return
        const {x, y, z} = landmarks[0][NOSE_TIP]
        //console.log(res)
        const x_nose = map_range(x, 1 , 0, 0, canvasElement.width)
        const y_nose = map_range(y, 1, 0, 0, canvasElement.height)

        const faceUp = landmarks[0][FACE_UP]
        const faceDown = landmarks[0][FACE_DOWN]

        if (!faceUp.y || !faceDown.y) {
            sunSize =  0.6
        } else {
            sunSize = faceDown.y -faceUp.y
        }

        //sunSize = map_range(z, -0.03, -0.12, SUN_SIZE_MIN, SUN_SIZE_MIN)
        headPosition = [x_nose, y_nose]
    }



    function removeWaitingMessage(){
        const msg = document.getElementById('wait')
        msg.remove()
    }

    function initDemo(video, w, h){
        if (navigator.mediaDevices && navigator.mediaDevices.getUserMedia ) {
            initVideo();
        } else {
            removeWaitingMessage()
            console.error('MediaDevices interface not available. Use the mouse as input')
            addMouseListeners()
            //playMouseDemo()
        }
    }


    async function initVideo(){
        createFaceLandmarker().then(function(){
                //const constraints = {video: {width: w, height: h, facingMode: 'user'}}
                const constraints = {video: true}
                navigator.mediaDevices.getUserMedia(constraints).then(function(stream) {
                    // apply the stream to the video element used in the texture
                    video.srcObject = stream
                    //video.play()
                    video.addEventListener("loadeddata", predictWebcam);
                    removeWaitingMessage()

                    //run()
                })
                // .catch(function (error){
                //     removeWaitingMessage()
                //     console.error('Unable to access the camera/webcam.', error)
                //     addMouseListeners()
                //     //playMouseDemo()
                // })
            }
        );
    }

let lastVideoTime = -1;
let results = undefined;

async function predictWebcam() {
  const ratio = video.VIDEO_HEIGHT / video.VIDEO_WIDTH;
  video.style.width = VIDEO_WIDTH + "px";
  video.style.height = VIDEO_WIDTH * ratio + "px";
  canvasElement.style.width = VIDEO_WIDTH + "px";
  canvasElement.style.height = VIDEO_WIDTH * ratio + "px";
  canvasElement.width = video.videoWidth;
  canvasElement.height = video.videoHeight;
  //Now let's start detecting the stream.
  let startTimeMs = performance.now();
  if (lastVideoTime !== video.currentTime) {
    lastVideoTime = video.currentTime;
    results = faceLandmarker.detectForVideo(video, startTimeMs);
  }
  if (results.faceLandmarks) {
    for (const landmarks of results.faceLandmarks) {
        console.log(landmarks);
    //   drawingUtils.drawConnectors(
    //     landmarks,
    //     FaceLandmarker.FACE_LANDMARKS_TESSELATION,
    //     { color: "#C0C0C070", lineWidth: 1 }
    //   );
    }
  }
  //drawBlendShapes(videoBlendShapes, results.faceBlendshapes);

  // Call this function again to keep predicting when the browser is ready.
    window.requestAnimationFrame(predictWebcam);
  
}

    function addMouseListeners(){
        addEventListener("mousemove", (event) => {
            headPosition = [event.clientX, (canvasElement.height - event.clientY)]        
        })
        addEventListener("wheel", (event) => {
            const inc = event.wheelDelta / 1200
            if (inc > 0 && sunSize + inc < SUN_SIZE_MAX){
                sunSize += inc
            } else if (inc < 0 && sunSize - inc > SUN_SIZE_MIN){
                sunSize += inc
            }
        })
    }


    </script>
</head>
<body>
    <div id="wait">Wait for camera to be detected, otherwise use the mouse as input</div>
    <video id="video" autoplay playsinline></video>
    <canvas id="facecanvas"></canvas>
</body>
</html>