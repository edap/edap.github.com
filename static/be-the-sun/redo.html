<!doctype html>
<html lang="en">

<head>
    <style>
        body{
            background-color: black;
            margin: 0px; padding: 0px; overflow: hidden;
        }
        #wait{
            color:white;
            font-family: system-ui;
            font-size: 42px;
            z-index: 3000;
            text-align: center;
            position: absolute;
            width: 100%;
            padding-top: 15%;
        }
        #video {
            position:absolute;
            top: -0;
            left: 0;
            right: 0;
            bottom: 0;
        }
    </style>
    <meta charset="utf-8">
    <title>Be the sun</title>
    <meta name="description" content="Be The Sun">
    <meta name="author" content="Davide Prati">
    <script src="./js/regl.min.js"></script>


    <script type="module">
    // 2017 first shadertoy version https://www.shadertoy.com/view/4lsyzn
    // 2017 webcam version online at edapx.neocities.org. defunct
    // 2018 online at davideprati.com, using headtracker.js
    // 2022 update. Use clmtracker.js
    // 2024 update. Use google mediapipe
    import vision from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3";
    const { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;

    const FACE_UP = 10
    const FACE_DOWN = 152
    const NOSE_TIP = 1
    const VIDEO_WIDTH = 300
    const VIDEO_HEIGHT = 225
    const SUN_SIZE_MAX = 1.0
    const SUN_SIZE_MIN = 0.2
    const DEBUG = true;

    let faceLandmarker;
    let runningMode = "VIDEO";
    let headPosition = [200.0, 200.0]
    let sunSize = 0.6
    const video = document.getElementById('video')
    const canvasElement = document.getElementById('facecanvas')
    canvasElement.width = window.innerWidth
    canvasElement.height = window.innerHeight
    const regl = createREGL({canvas: canvasElement})
    const drawTriangle = regl({
        frag: `
            precision mediump float;
            uniform float uSunSize;
            uniform vec2 iResolution;
            uniform vec2 uHead;

            float circleSmooth(in vec2 st, in vec2 pos, in float begin, in float end) {
                float pct = 0.0;
                pct = 1. - smoothstep(begin, end, distance(st, pos));
                return pct;
            }

            float rectangleGradientBottom(in vec2 st, in vec2 origin, in vec2 dimensions, float smoothness) {
                vec2 center = step(origin, st); // it is actually the bottom left cornter
                float pct = center.x * center.y;
                vec2 full = step(1.0 - origin - dimensions, 1.0 - st);
                float height = origin.y+dimensions.y;
                pct *= full.x * full.y;
                pct *= smoothstep(height, origin.y+smoothness,st.y);
                return pct;
            }

            //  Function from IÃ±igo Quiles
            //  www.iquilezles.org/www/articles/functions/functions.htm
            float parabola( float x, float k ){
                return pow( 4.0*x*(1.0-x), k );
            }

            void main() {
                vec2 uv = gl_FragCoord.xy / iResolution.xy;
                vec2 headNorm = uHead/iResolution;
                // use parabola function to get the y coordinate
                //vec2 m = vec2(headNorm.x, parabola(headNorm.x, 1.2));
                vec2 m = vec2(headNorm.x, headNorm.y);
                vec3 color = vec3(0.1, 0.4, 0.9);
                color += vec3(0.9-m.y,0.6-uv.y, -(1.0-m.y));
                uv.x *= iResolution.x / iResolution.y;
                m.x *= iResolution.x / iResolution.y;
                vec2 recPos = vec2(0.0, 0.0);

                // kind of sun, with a lot of imagination
                color.rg += circleSmooth(uv, vec2(m), 0.05, 0.15 + uSunSize);
                float turnOffSky = smoothstep(0.25, 0.27,m.y);
                color*= turnOffSky;

                // earth
                vec3 earth = vec3(vec2(rectangleGradientBottom(uv, recPos, vec2(40.0, 0.3), 0.16)), 0.0);
                earth.r += 1.0 -m.y;
                color += earth;
                float turnOffEarth = smoothstep(0.09, 0.25,m.y);
                color*= turnOffEarth;

                gl_FragColor = vec4(color, 1.0);
            }`,

        vert: `
            precision mediump float;
            attribute vec3 position;

            void main() {
                gl_Position = vec4(position, 1);
            }`,

        attributes: {
            position: regl.buffer([
                [[-1, -1, 0], [-1, 1, 0], [1, 1, 0]],
                [[1, 1, 0], [1, -1, 0], [-1, -1, 0]]
            ])
        },

        uniforms: {
            iResolution: regl.prop('resolution'),
            uSunSize: regl.prop('sunSize'),
            uHead: regl.prop('headPosition')
        },
        count: 6
    })   

    if (!DEBUG) {
        video.setAttribute("hidden", "hidden")
    }


    initDemo(video, VIDEO_WIDTH, VIDEO_HEIGHT)

    // Before we can use FaceLandmarker class we must wait for it to finish
    // loading. Machine Learning models can be large and take a moment to
    // get everything needed to run.
    async function createFaceLandmarker() {
        const filesetResolver = await FilesetResolver.forVisionTasks(
            "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3/wasm"
        );
        faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {
            baseOptions: {
            modelAssetPath: `https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task`,
            delegate: "GPU"
            },
            outputFaceBlendshapes: true,
            runningMode,
            numFaces: 1
        });
    };



    function map_range(value, low1, high1, low2, high2) {
        let mapped = low2 + (high2 - low2) * (value - low1) / (high1 - low1)
        return Math.min(Math.max(mapped, low2), high2)
    }

    const setSunPosition = function(landmarks){
        //const landmarks = res.multiFaceLandmarks
        if(!landmarks) return
        const {x, y, z} = landmarks[NOSE_TIP]
        //console.log(res)
        const x_nose = map_range(x, 1 , 0, 0, canvasElement.width)
        const y_nose = map_range(y, 1, 0, 0, canvasElement.height)

        const faceUp = landmarks[FACE_UP]
        const faceDown = landmarks[FACE_DOWN]

        if (!faceUp.y || !faceDown.y) {
            sunSize =  0.6
        } else {
            sunSize = faceDown.y -faceUp.y
        }

        //sunSize = map_range(z, -0.03, -0.12, SUN_SIZE_MIN, SUN_SIZE_MIN)
        headPosition = [x_nose, y_nose]
    }



    function removeWaitingMessage(){
        const msg = document.getElementById('wait')
        msg.remove()
    }

    function initDemo(video, w, h){
        if (navigator.mediaDevices && navigator.mediaDevices.getUserMedia ) {
            initVideo();
        } else {
            removeWaitingMessage()
            console.error('MediaDevices interface not available. Use the mouse as input')
            addMouseListeners()
            //playMouseDemo()
        }
    }


    async function initVideo(){
        createFaceLandmarker().then(function(){
                //const constraints = {video: {width: w, height: h, facingMode: 'user'}}
                const constraints = {video: true}
                navigator.mediaDevices.getUserMedia(constraints).then(function(stream) {
                    // apply the stream to the video element used in the texture
                    video.srcObject = stream
                    //video.play()
                    video.addEventListener("loadeddata", predictWebcam);
                    removeWaitingMessage()
                })
                .catch(function (error){
                    removeWaitingMessage()
                    console.error('Unable to access the camera/webcam.', error)
                    addMouseListeners()
                    //playMouseDemo()
                })
            }
        );
    }

let lastVideoTime = -1;
let results = undefined;

async function predictWebcam() {
  const ratio = video.VIDEO_HEIGHT / video.VIDEO_WIDTH;
  video.style.width = VIDEO_WIDTH + "px";
  video.style.height = VIDEO_WIDTH * ratio + "px";
//   canvasElement.style.width = VIDEO_WIDTH + "px";
//   canvasElement.style.height = VIDEO_WIDTH * ratio + "px";
//   canvasElement.width = video.videoWidth;
//   canvasElement.height = video.videoHeight;
  //Now let's start detecting the stream.
  let startTimeMs = performance.now();
  if (lastVideoTime !== video.currentTime) {
    lastVideoTime = video.currentTime;
    results = faceLandmarker.detectForVideo(video, startTimeMs);
  }
  if (results.faceLandmarks) {
    for (const landmarks of results.faceLandmarks) {
        setSunPosition(landmarks)
        //console.log(landmarks);
    //   drawingUtils.drawConnectors(
    //     landmarks,
    //     FaceLandmarker.FACE_LANDMARKS_TESSELATION,
    //     { color: "#C0C0C070", lineWidth: 1 }
    //   );
    }
  }
  //drawBlendShapes(videoBlendShapes, results.faceBlendshapes);

  // Call this function again to keep predicting when the browser is ready.
  regl.frame(function (context) {
            drawTriangle({
                resolution: [context.drawingBufferWidth, context.drawingBufferHeight],
                sunSize: sunSize,
                headPosition: headPosition
            })
        })
    window.requestAnimationFrame(predictWebcam);
  
}



function addMouseListeners(){
    addEventListener("mousemove", (event) => {
        headPosition = [event.clientX, (canvasElement.height - event.clientY)]        
    })
    addEventListener("wheel", (event) => {
        const inc = event.wheelDelta / 1200
        if (inc > 0 && sunSize + inc < SUN_SIZE_MAX){
            sunSize += inc
        } else if (inc < 0 && sunSize - inc > SUN_SIZE_MIN){
            sunSize += inc
        }
    })
}



 


    </script>
</head>
<body>
    <div id="wait">Wait for camera to be detected, otherwise use the mouse as input</div>
    <video id="video" autoplay playsinline></video>
    <canvas id="facecanvas"></canvas>
</body>
</html>