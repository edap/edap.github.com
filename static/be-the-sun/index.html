<!doctype html>
<html lang="en">

<head>
    <style>
        body {
            background-color: black;
            margin: 0px;
            padding: 0px;
            overflow: hidden;
        }

        #wait {
            color: white;
            font-family: system-ui;
            font-size: 42px;
            z-index: 3000;
            text-align: center;
            position: absolute;
            width: 100%;
            padding-top: 15%;
        }

        #video {
            position: absolute;
            top: -0;
            left: 0;
            right: 0;
            bottom: 0;
        }
    </style>
    <meta charset="utf-8">
    <title>Be the sun</title>
    <meta name="description" content="Be The Sun">
    <meta name="author" content="Davide Prati">
    <script src="./js/regl.min.js"></script>

    <script type="module">
        // 2017 first shadertoy version https://www.shadertoy.com/view/4lsyzn
        // 2017 webcam version online at edapx.neocities.org. defunct
        // 2018 online at davideprati.com, using headtracker.js
        // 2022 update. Use clmtracker.js
        // 2024 update. Use google mediapipe
        import vision from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3"
        const { FaceLandmarker, FilesetResolver, DrawingUtils } = vision

        const FACE_UP = 10
        const FACE_DOWN = 152
        const NOSE_TIP = 1
        const VIDEO_WIDTH = 300
        const VIDEO_HEIGHT = 225
        const DEBUG = false

        let faceLandmarker
        let headPosition = [200.0, 200.0]
        let sunSize = 0.6

        const video = document.getElementById('video')
        const canvasElement = document.getElementById('facecanvas')
        canvasElement.width = window.innerWidth
        canvasElement.height = window.innerHeight

        const regl = createREGL({ canvas: canvasElement })

        const drawTriangle = regl({
            frag: `
            precision mediump float;
            uniform float uSunSize;
            uniform vec2 uResolution;
            uniform vec2 uHead;

            float circleSmooth(in vec2 st, in vec2 pos, in float begin, in float end) {
                float pct = 0.0;
                pct = 1. - smoothstep(begin, end, distance(st, pos));
                return pct;
            }

            float rectangleGradientBottom(in vec2 st, in vec2 origin, in vec2 dimensions, float smoothness) {
                vec2 center = step(origin, st); // it is actually the bottom left cornter
                float pct = center.x * center.y;
                vec2 full = step(1.0 - origin - dimensions, 1.0 - st);
                float height = origin.y+dimensions.y;
                pct *= full.x * full.y;
                pct *= smoothstep(height, origin.y+smoothness,st.y);
                return pct;
            }

            void main() {
                vec2 uv = gl_FragCoord.xy / uResolution.xy;
                vec2 headNorm = uHead/uResolution;
                vec2 m = vec2(headNorm.x, headNorm.y);
                vec3 color = vec3(0.1, 0.4, 0.9);
                color += vec3(0.9-m.y,0.6-uv.y, -(1.0-m.y));
                uv.x *= uResolution.x / uResolution.y;
                m.x *= uResolution.x / uResolution.y;
                vec2 recPos = vec2(0.0, 0.0);

                // sun
                color.rg += circleSmooth(uv, vec2(m), 0.05, 0.15 + uSunSize);
                float turnOffSky = smoothstep(0.25, 0.27,m.y);
                color*= turnOffSky;

                // earth
                vec3 earth = vec3(vec2(rectangleGradientBottom(uv, recPos, vec2(40.0, 0.3), 0.16)), 0.0);
                earth.r += 1.0 -m.y;
                color += earth;
                float turnOffEarth = smoothstep(0.09, 0.25,m.y);
                color*= turnOffEarth;

                gl_FragColor = vec4(color, 1.0);
            }`,

            vert: `
            precision mediump float;
            attribute vec3 position;

            void main() {
                gl_Position = vec4(position, 1);
            }`,

            attributes: {
                position: regl.buffer([
                    [[-1, -1, 0], [-1, 1, 0], [1, 1, 0]],
                    [[1, 1, 0], [1, -1, 0], [-1, -1, 0]]
                ])
            },

            uniforms: {
                uResolution: regl.prop('resolution'),
                uSunSize: regl.prop('sunSize'),
                uHead: regl.prop('headPosition')
            },
            count: 6
        })

        const renderScene = () => {
            const context = regl._gl
            regl.clear({
                color: [0, 0, 0, 1],
                depth: 1,
            })
            drawTriangle({
                resolution: [context.drawingBufferWidth, context.drawingBufferHeight],
                sunSize: sunSize,
                headPosition: headPosition
            })
        }

        if (!DEBUG) {
            video.setAttribute("hidden", "hidden")
        }

        const initDemo = (video, w, h) => {
            if (navigator.mediaDevices && navigator.mediaDevices.getUserMedia && !isMobile) {
                initVideo()
            } else {
                removeWaitingMessage()
                console.error('MediaDevices interface not available. Use the mouse as input')
                addPointerListeners()
                playMouseDemo()
            }
        }

        // Before we can use FaceLandmarker class we must wait for it to finish
        // loading. Machine Learning models can be large and take a moment to
        // get everything needed to run.
        const createFaceLandmarker = async () => {
            const filesetResolver = await FilesetResolver.forVisionTasks(
                "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3/wasm"
            )
            faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {
                baseOptions: {
                    modelAssetPath: `https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task`,
                    delegate: "GPU"
                },
                outputFaceBlendshapes: true,
                runningMode: "VIDEO",
                numFaces: 1
            })
        }

        const map_range = (value, low1, high1, low2, high2) => {
            let mapped = low2 + (high2 - low2) * (value - low1) / (high1 - low1)
            return Math.min(Math.max(mapped, low2), high2)
        }

        const initVideo = async () => {
            createFaceLandmarker().then(function () {
                const constraints = { video: true }
                navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {
                    video.srcObject = stream
                    video.addEventListener("loadeddata", predictWebcam)
                    removeWaitingMessage()
                })
                    .catch(function (error) {
                        removeWaitingMessage()
                        console.error('Unable to access the camera/webcam.', error)
                        addPointerListeners()
                        playMouseDemo()
                    })
            }
            )
        }

        let lastVideoTime = -1
        let results = undefined
        const predictWebcam = async () => {
            const ratio = video.VIDEO_HEIGHT / video.VIDEO_WIDTH
            video.style.width = VIDEO_WIDTH + "px"
            video.style.height = VIDEO_WIDTH * ratio + "px"
            let startTimeMs = performance.now()
            if (lastVideoTime !== video.currentTime) {
                lastVideoTime = video.currentTime
                results = faceLandmarker.detectForVideo(video, startTimeMs)
            }
            if (results.faceLandmarks) {
                for (const landmarks of results.faceLandmarks) {
                    setSunPositionAndSize(landmarks)
                }
            }

            renderScene()
            // Call this function again to keep predicting when the browser is ready.
            window.requestAnimationFrame(predictWebcam)
        }

        const addPointerListeners = () => {
            canvasElement.addEventListener("pointermove", (event) => {
                headPosition = [event.clientX, (canvasElement.height - event.clientY)]
            }, false)

            addEventListener("wheel", (event) => {
                const inc = event.wheelDelta / 1200
                if (inc > 0 && sunSize + inc < SUN_SIZE_MAX) {
                    sunSize += inc
                } else if (inc < 0 && sunSize - inc > SUN_SIZE_MIN) {
                    sunSize += inc
                }
            })
        }

        const playMouseDemo = () => {
            renderScene()
            requestAnimationFrame(playMouseDemo)
        }

        const setSunPositionAndSize = (landmarks) => {
            if (!landmarks) return
            const { x, y, z } = landmarks[NOSE_TIP]
            const x_nose = map_range(x, 1, 0, 0, canvasElement.width)
            const y_nose = map_range(y, 1, 0, 0, canvasElement.height)

            const faceUp = landmarks[FACE_UP]
            const faceDown = landmarks[FACE_DOWN]

            if (!faceUp.y || !faceDown.y) {
                sunSize = 0.6
            } else {
                sunSize = faceDown.y - faceUp.y
            }

            headPosition = [x_nose, y_nose]
        }

        const removeWaitingMessage = () => {
            const msg = document.getElementById('wait')
            msg.remove()
        }

        const isMobile = () => {
            let mobile = false;

            if (navigator.userAgent.match(/Android/i)
                || navigator.userAgent.match(/webOS/i)
                || navigator.userAgent.match(/iPhone/i)
                || navigator.userAgent.match(/iPad/i)
                || navigator.userAgent.match(/iPod/i)
                || navigator.userAgent.match(/BlackBerry/i)
                || navigator.userAgent.match(/Windows Phone/i)) {
                mobile = true
            }
            return mobile
        }


        initDemo(video, VIDEO_WIDTH, VIDEO_HEIGHT)

    </script>
</head>

<body>
    <div id="wait">Wait for camera to be detected, otherwise use the mouse as input</div>
    <video id="video" autoplay playsinline></video>
    <canvas id="facecanvas"></canvas>
</body>

</html>